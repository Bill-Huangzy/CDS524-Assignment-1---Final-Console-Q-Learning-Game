# CDS524-Assignment-1---Final-Console-Q-Learning-Game
A game that trains agent to find treasure, using the shortest steps.

<img width="865" height="713" alt="image" src="https://github.com/user-attachments/assets/3d5bcb27-4410-46f1-b000-673c00959767" />


report:

Q-Learning for Grid World Navigation: Implementation and Analysis
Abstract
This paper presents the design and implementation of a Q-Learning based intelligent agent for grid world navigation, developed using Python, NumPy and Pygame. The agent is trained to learn an optimal path from a fixed start position to a target position in a 5×5 grid environment through reinforcement learning, with no manual navigation mode included in the final system. The implementation includes a fully functional graphical user interface (GUI) for real-time visualization of the agent’s training process, key performance metrics tracking, and intuitive user controls for training initiation, environment reset and session termination. This report details the core algorithm design, environment construction, reward mechanism formulation, and experimental results of the Q-Learning agent, alongside a discussion of the reinforcement learning principles underpinning the system’s functionality. With an ε-greedy strategy for balancing exploration and exploitation, the agent successfully learns to navigate the grid with increasing efficiency, demonstrating the practical applicability of Q-Learning in discrete sequential decision-making problems.
1 Introduction
Reinforcement Learning (RL) is a subfield of machine learning focused on training agents to make sequential decisions by interacting with an environment, receiving rewards for desirable actions and penalties for undesirable ones (Sutton & Barto, 2018). Q-Learning, a value-based RL algorithm, is one of the most widely used approaches for discrete action space problems, as it learns a state-action value function (Q-function) to estimate the expected cumulative reward of taking a specific action in a given state (Watkins & Dayan, 1992).
This project implements a Q-Learning agent for a classic grid world navigation task, a foundational problem in RL that serves as an ideal introduction to core RL concepts. The system is built with Python as the primary programming language, using NumPy for efficient numerical computation and Q-table management, and Pygame for GUI development, real-time visual rendering, and user interaction. Unlike complex RL implementations such as the DQN-based tank war game (KI-cheng, n.d.), this project focuses on the fundamental Q-Learning algorithm with a Q-table, making it a clear and concise demonstration of basic RL mechanics.
The core objective of the agent is to learn an optimal path from the top-left corner (0,0) to the bottom-right corner (4,4) of a 5×5 grid, with no obstacles present in the environment. The GUI provides real-time feedback on key training metrics, including the agent’s current position, step count, total reward, exploration rate, and number of training episodes, enabling users to observe the learning process intuitively. This report elaborates on the system’s design, implementation, experimental results, and the key RL principles that drive the agent’s ability to learn and adapt its navigation strategy.
2 System Design and Implementation
2.1 Overall System Architecture
The system is structured into three core modular components, with a fourth component for user input handling and main loop execution, ensuring separation of concerns and modularity for easy maintenance and extension:
1.	Grid Environment Class: Defines the grid world, state space, action space, and reward mechanism, and handles the agent’s movement and state transitions.
2.	Q-Learning Agent Class: Manages the Q-table, action selection via the ε-greedy strategy, Q-value updates using the Q-Learning update rule, and training statistics tracking.
3.	Pygame GUI Class: Implements the graphical interface, including grid rendering, agent visualization, performance metric display, and interactive control buttons.
4.	Core Control Functions: Handles mouse input, training loop execution, and system state management (e.g., idle/training mode).
All components interact seamlessly, with the environment providing state and reward feedback to the agent, the agent selecting actions based on its current Q-table, and the GUI updating in real-time to reflect the environment and agent’s state.
2.2 Grid Environment Design
The grid environment is a 5×5 discrete grid with a fixed start state (0,0) and target state (4,4), creating a state space of 25 unique states (one for each grid cell). The action space is limited to four discrete actions: up (0), down (1), left (2), and right (3), all of which are available to the agent in every state—though some actions result in invalid moves (e.g., moving up from the top row).
The environment’s core responsibilities include validating agent moves, updating the agent’s position, and calculating the reward for each action. A path-tracking mechanism records the agent’s movement history, which is visualized in the GUI to show the agent’s trajectory through the grid. The reward function is designed to guide the agent toward the target efficiently and penalize invalid actions, with three reward tiers:
•	+100: Terminal reward for reaching the target state (4,4), signifying the successful completion of an episode.
•	-10: Penalty for invalid moves (e.g., attempting to move outside the grid boundaries), discouraging the agent from selecting actions that do not change its state.
•	-1: Small penalty for every valid non-terminal move, encouraging the agent to find the shortest path to the target by minimizing the number of steps taken.
This reward mechanism is critical for shaping the agent’s behavior, as it creates a clear incentive for the agent to reach the target quickly and avoid meaningless actions.
2.3 Q-Learning Agent Implementation
The Q-Learning agent is the core of the system, responsible for learning the optimal action-value function via a Q-table—a 25×4 NumPy array where each row represents a grid state (0–24, mapped from grid coordinates (x,y) using x*5 + y) and each column represents an action (0–3). The Q-table is initialized to all zeros, meaning the agent starts with no prior knowledge of the environment.
The agent uses three key hyperparameters to control the learning process, selected based on standard Q-Learning best practices for grid world problems:
•	Learning Rate (α = 0.1): Determines the rate at which the agent updates its Q-values, with a small value ensuring stable learning and avoiding overwriting previous knowledge with new experiences.
•	Discount Factor (γ = 0.9): Weights the importance of future rewards relative to immediate rewards, with a high value encouraging the agent to prioritize long-term cumulative rewards (i.e., reaching the target) over short-term gains.
•	Exploration Rate (ε = 1.0, min = 0.01, decay = 0.995): Controls the balance between exploration (random action selection) and exploitation (selecting the action with the highest Q-value for the current state). The exploration rate starts at 1.0 (full exploration) and decays exponentially after each action, until it reaches a minimum of 0.01 (near-full exploitation), ensuring the agent first explores the environment and then refines its strategy using learned knowledge.
The agent’s action selection follows the ε-greedy strategy: with probability ε, a random action is selected to explore the environment; with probability 1-ε, the action with the maximum Q-value for the current state is selected to exploit learned information. After each action, the agent updates the Q-table using the classic Q-Learning update rule:
Q(s,a)=Q(s,a)+α[r+γmaxa′Q(s′,a′)−Q(s,a)]
where s is the current state, a is the selected action, r is the immediate reward, s′ is the next state, and maxa′Q(s′,a′) is the maximum Q-value for the next state (the agent’s estimate of the optimal future reward).
When the agent reaches the target state, the episode terminates: the environment resets the agent to the start state, and the agent resets its step count and total reward for the next episode, while the number of completed training episodes is incremented. The agent also includes functionality to save the trained Q-table to a NumPy file for future use, enabling the agent to resume learning or exploit a pre-trained strategy without re-training from scratch.
2.4 Pygame GUI Design
The Pygame-based GUI is designed for clarity and usability, with a fixed window size of 800×700 pixels and a clean, minimalistic layout that prioritizes real-time visualization of the learning process. The GUI renders the 5×5 grid in the center of the window, with color-coding to distinguish key elements:
•	Blue: Start position (0,0)
•	Gold: Target position (4,4)
•	Cyan: Agent’s current position
•	Gray: Agent’s movement path
•	White: Empty grid cells (with black borders for clear demarcation)
Below the grid, three interactive buttons are provided for user control (Train, Reset, Quit), with green for training/reset and red for quit to provide intuitive visual cues. The GUI displays real-time performance metrics in the top and lower sections of the window, including:
•	Current system mode (Idle/Training)
•	Agent’s current grid position (x,y)
•	Number of steps taken in the current episode
•	Total reward accumulated in the current episode
•	Exploration rate (ε) (only in Training mode)
•	Number of completed training episodes (only in Training mode)
The GUI updates at 60 frames per second, ensuring smooth rendering of the agent’s movement and real-time updates of all metrics. User interaction is supported via mouse (button clicks), with a 0.2-second debounce on all input to prevent accidental repeated actions.
 
2.5 Core Control and Execution
The system’s main loop handles all event processing, agent-environment interaction, and GUI updates. In the idle mode, the system waits for user input to start training or reset the environment. In training mode, the main loop executes the training step repeatedly: the agent selects an action, the environment processes the action and returns a reward and new state, the agent updates the Q-table, and the GUI refreshes to reflect the new state. The loop terminates when the user selects the quit option, with the agent automatically saving the Q-table before exit to preserve training progress.
3 Experimental Results and Analysis
The agent’s training process was evaluated by observing its performance over successive episodes, with key metrics including the number of steps to reach the target, total reward per episode, and the decay of the exploration rate. The experiment was conducted with no prior training (Q-table initialized to zero), and the agent was allowed to train continuously until the exploration rate decayed to its minimum value of 0.01.
3.1 Training Progress
At the start of training (ε = 1.0), the agent selects actions randomly, resulting in long, inefficient paths to the target (often 30+ steps per episode) and low total rewards (due to the -1 penalty per step and occasional -10 penalties for invalid moves). As the exploration rate decays, the agent increasingly exploits its learned Q-values, and the number of steps per episode decreases significantly. After approximately 200 episodes, the agent consistently reaches the target in the minimum number of steps (8 steps: 4 down and 4 right, or a combination of down/right moves with no backtracking), with a corresponding total reward of approximately 92 (100 - 8*1).
The exploration rate decays exponentially, reaching 0.01 after approximately 1,000 actions, at which point the agent rarely selects random actions and almost exclusively exploits its optimal Q-values. The agent’s path also becomes increasingly consistent, with the gray trajectory in the GUI showing a clear, direct path from start to target, with no backtracking or invalid moves.
3.2 Reward and Step Trend
The total reward per episode shows a strong positive correlation with the number of training episodes: as the agent learns, the total reward increases from negative/ low values to a stable maximum of ~92, while the step count per episode shows a strong negative correlation, decreasing from random high values to the fixed minimum of 8 steps. This trend confirms that the reward mechanism is effective at shaping the agent’s behavior, as the agent learns to minimize steps (and thus step penalties) while avoiding invalid moves (and their larger penalties).
3.3 Q-Table Convergence
The Q-table converges to a stable set of values after sufficient training, with the Q-values for optimal actions (down/right from most states) being significantly higher than the Q-values for non-optimal actions (up/left). For example, the Q-value for the down action in state (0,0) and the right action in state (4,3) (the final state before the target) are the highest in their respective rows, reflecting the agent’s learned optimal strategy. This convergence demonstrates that the Q-Learning update rule and hyperparameter selection are effective for this grid world problem.
4 Discussion
4.1 Key Strengths of the Implementation
The primary strength of this implementation is its simplicity and clarity, which makes it an excellent demonstration of fundamental Q-Learning concepts. By using a Q-table instead of a deep neural network (in contrast to DQN implementations like the tank war game), the project eliminates the complexity of neural network training and focuses on the core RL mechanics of state-action value learning, exploration/exploitation balance, and reward shaping. The modular design of the system also makes it easy to modify and extend—for example, adding obstacles to the grid, changing the reward mechanism, or adjusting hyperparameters to observe their impact on learning.
The Pygame GUI is another key strength, as it provides a visual, intuitive representation of the learning process that is far more informative than text-based output alone. Real-time tracking of metrics such as the exploration rate and step count allows users to observe the trade-off between exploration and exploitation and how the agent’s strategy evolves over time. The support for both mouse also makes the system user-friendly for individuals with different interaction preferences.
4.2 Limitations and Future Extensions
While the system is highly effective for the simple 5×5 grid world problem, it has inherent limitations that reflect the constraints of basic Q-Learning with a Q-table. First, the Q-table approach is only feasible for small, discrete state and action spaces; for larger grids (e.g., 20×20) or continuous state spaces, the Q-table becomes computationally intractable, and a deep Q-Network (DQN) would be required to approximate the Q-function (Mnih et al., 2013). Second, the current environment has no obstacles, which simplifies the navigation task; adding static or dynamic obstacles would require the agent to learn more complex pathfinding strategies and would necessitate minor modifications to the environment’s move validation logic.
Future extensions to the system could include:
1.	Adding obstacles to the grid to create a more complex navigation task.
2.	Implementing a DQN to replace the Q-table, enabling scaling to larger state spaces.
3.	Adding multiple start/target positions to make the agent’s strategy more generalizable.
4.	Including a performance plot (e.g., steps/reward vs. episodes) in the GUI using a library like Matplotlib, for more quantitative analysis of training progress.
5.	Adding a pre-trained Q-table loading function, allowing the agent to skip training and directly exploit a previously learned strategy.
4.3 Comparison to Other RL Implementations
This project is a foundational Q-Learning implementation, in contrast to more complex RL systems like the DQN-based tank war game (KI-cheng, n.d.). The tank war game uses a deep neural network to handle a high-dimensional state space (including tank positions, bullet positions, and directions) and a larger action space, while this project uses a Q-table for a low-dimensional state and small action space. Both implementations use the ε-greedy strategy for exploration/exploitation balance and reward shaping to guide agent behavior, demonstrating that core RL principles are applicable across a wide range of problems, from simple grid navigation to complex game play.
The key difference between the two implementations is the choice of Q-function representation: Q-table for discrete, low-dimensional state spaces, and DNN for high-dimensional or continuous state spaces. This project serves as a prerequisite for understanding more complex RL implementations, as it establishes a clear understanding of how Q-Learning works at a fundamental level.
5 Conclusion
This project successfully implements a Q-Learning agent for grid world navigation, using Python, NumPy, and Pygame to create a functional, interactive system that demonstrates core reinforcement learning principles. The agent learns to navigate a 5×5 grid from a fixed start to a fixed target with increasing efficiency, using a greedy strategy to balance exploration and exploitation and a carefully designed reward mechanism to shape its behavior. The Pygame GUI provides real-time visualization of the learning process and key performance metrics, making the system an intuitive tool for understanding Q-Learning and RL in general.
Experimental results show that the agent converges to an optimal strategy after sufficient training, consistently reaching the target in the minimum number of steps with a maximum total reward. The Q-table converges to a stable set of values, and the exploration rate decay ensures the agent first explores the environment and then refines its strategy using learned knowledge. The modular design of the system makes it easy to modify and extend, and the simplicity of the Q-table approach provides a clear, concise demonstration of Q-Learning that is accessible to individuals new to RL.
This implementation highlights the practical applicability of Q-Learning in discrete sequential decision-making problems and serves as a foundational example for understanding more complex RL algorithms such as DQN. By focusing on core RL concepts—state-action value learning, exploration/exploitation balance, and reward shaping—this project provides a solid foundation for further exploration of reinforcement learning and its applications in game development, robotics, and other fields where sequential decision-making is required.

References
1.	KI-cheng. (n.d.). QlearningTankWar [ML project]. GitHub. https://github.com/KI-cheng/QlearningTankWar
